# LLM Proxy Server

–ü—Ä–æ–∫—Å—ñ-—Å–µ—Ä–≤–µ—Ä –¥–ª—è llama-cpp-python, —è–∫–∏–π –ø—Ä–∏–π–º–∞—î –∑–∞–ø–∏—Ç–∏ —Ç–∞ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è—î —ó—Ö –Ω–∞ llama-cpp-python server.

## –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞

```
–ö–ª—ñ—î–Ω—Ç -> FastAPI Proxy (port 8080) -> llama-cpp-python server (port 8000) -> Model
```

## –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è

### üéØ –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è Python 3.11 (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ —Å–ø–æ—á–∞—Ç–∫—É)

–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è Python 3.11 –∑ –≤—ñ—Ä—Ç—É–∞–ª—å–Ω–∏–º —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ–º:

**PowerShell (Windows):**
```powershell
.\setup_python311.ps1
```

**Bash (Linux/WSL):**
```bash
chmod +x setup_python311.sh
./setup_python311.sh
```

–°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ:
- ‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä—è—î –Ω–∞—è–≤–Ω—ñ—Å—Ç—å Python 3.11
- ‚úÖ –ü—Ä–æ–ø–æ–Ω—É—î –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ Python 3.11 —è–∫—â–æ –≤—ñ–¥—Å—É—Ç–Ω—ñ–π
- ‚úÖ –°—Ç–≤–æ—Ä—é—î –≤—ñ—Ä—Ç—É–∞–ª—å–Ω–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ `.venv`
- ‚úÖ –û–Ω–æ–≤–ª—é—î pip, setuptools, wheel
- ‚úÖ –í—Å—Ç–∞–Ω–æ–≤–ª—é—î –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –∑ requirements.txt

–ü—ñ—Å–ª—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∞–∫—Ç–∏–≤—É–π—Ç–µ –≤—ñ—Ä—Ç—É–∞–ª—å–Ω–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ:
```powershell
# PowerShell
.\.venv\Scripts\Activate.ps1

# Bash
source .venv/bin/activate
```

### üîß –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–æ–µ–∫—Ç—É

–û–¥–Ω–∞ –∫–æ–º–∞–Ω–¥–∞ –¥–ª—è –ø–æ–≤–Ω–æ–≥–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞:

```bash
chmod +x setup.sh
./setup.sh
```

–°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ:
- ‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä—è—î Python —Ç–∞ pip
- ‚úÖ –ü—Ä–æ–ø–æ–Ω—É—î —Å—Ç–≤–æ—Ä–∏—Ç–∏ –≤—ñ—Ä—Ç—É–∞–ª—å–Ω–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ
- ‚úÖ –í—Å—Ç–∞–Ω–æ–≤–ª—é—î –≤—Å—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –∑ requirements.txt
- ‚úÖ –í—Å—Ç–∞–Ω–æ–≤–ª—é—î llama-cpp-python[server] (–∑ CUDA —è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–æ)
- ‚úÖ –ù–∞–ª–∞—à—Ç–æ–≤—É—î –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø—É –¥–æ —Å–∫—Ä–∏–ø—Ç—ñ–≤
- ‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä—è—î –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Ç–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó

### –†—É—á–Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è

**1. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –¥–ª—è –ø—Ä–æ–∫—Å—ñ-—Å–µ—Ä–≤–µ—Ä–∞:**
```bash
pip install -r requirements.txt
```

**2. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å llama-cpp-python –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é —Å–µ—Ä–≤–µ—Ä–∞:**
```bash
pip install llama-cpp-python[server]
```

**3. –ó –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é CUDA (GPU):**
```bash
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python[server] --upgrade --force-reinstall --no-cache-dir
```

## –ó–∞–ø—É—Å–∫

### üöÄ –®–≤–∏–¥–∫–∏–π —Å—Ç–∞—Ä—Ç (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)

–ó–∞–ø—É—Å—Ç—ñ—Ç—å –æ–±–∏–¥–≤–∞ —Å–µ—Ä–≤–µ—Ä–∏ –æ–¥–Ω–∏–º —Å–∫—Ä–∏–ø—Ç–æ–º:

```bash
chmod +x start.sh
./start.sh
```

–î–ª—è –∑—É–ø–∏–Ω–∫–∏ —Å–µ—Ä–≤–µ—Ä—ñ–≤:
```bash
chmod +x stop.sh
./stop.sh
```

–ê–±–æ –Ω–∞—Ç–∏—Å–Ω—ñ—Ç—å `Ctrl+C` –≤ —Ç–µ—Ä–º—ñ–Ω–∞–ª—ñ –¥–µ –∑–∞–ø—É—â–µ–Ω–æ `start.sh`.

### –í–∞—Ä—ñ–∞–Ω—Ç 2: –û–∫—Ä–µ–º—ñ —Ç–µ—Ä–º—ñ–Ω–∞–ª–∏

**–¢–µ—Ä–º—ñ–Ω–∞–ª 1 - Llama server:**
```bash
chmod +x start_llama_server.sh
./start_llama_server.sh
```

**–¢–µ—Ä–º—ñ–Ω–∞–ª 2 - Proxy server:**
```bash
chmod +x start_proxy_server.sh
./start_proxy_server.sh
```

### –í–∞—Ä—ñ–∞–Ω—Ç 3: –†—É—á–Ω–∏–π –∑–∞–ø—É—Å–∫

**–¢–µ—Ä–º—ñ–Ω–∞–ª 1:**
```bash
python3 -m llama_cpp.server \
  --model /home/kostanich/llama/models/mistral-7b-instruct-v0.3.Q4_K_M.gguf \
  --host 127.0.0.1 \
  --port 8000 \
  --n_gpu_layers 40 \
  --n_ctx 4096
```

**–¢–µ—Ä–º—ñ–Ω–∞–ª 2:**
```bash
uvicorn app_server:app --host 0.0.0.0 --port 8080 --reload
```

## –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è

–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≤ `config.json`:
- `llama_server_url` - URL llama-cpp-python —Å–µ—Ä–≤–µ—Ä–∞ (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: http://localhost:8000)
- `model` - –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è —Ç—ñ–ª—å–∫–∏ –¥–ª—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—ñ–≤)

## –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

–ù–∞–¥—ñ—à–ª—ñ—Ç—å POST –∑–∞–ø–∏—Ç –Ω–∞ `/extract`:

```bash
curl -X POST "http://localhost:8080/extract" \
  -F "text=–í–∞—à —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–æ–±–∫–∏"
```

–ê–±–æ —á–µ—Ä–µ–∑ Python:
```python
import requests

response = requests.post(
    "http://localhost:8080/extract",
    data={"text": "–í–∞—à —Ç–µ–∫—Å—Ç"}
)
print(response.json())
```

## Endpoints

- **POST /extract** - –û—Å–Ω–æ–≤–Ω–∏–π endpoint –¥–ª—è –æ–±—Ä–æ–±–∫–∏ —Ç–µ–∫—Å—Ç—É
- **GET /docs** - Swagger –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è API

## –ü–æ—Ä—Ç–∏

- `8000` - llama-cpp-python server (–≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ–π)
- `8080` - FastAPI –ø—Ä–æ–∫—Å—ñ —Å–µ—Ä–≤–µ—Ä (–∑–æ–≤–Ω—ñ—à–Ω—ñ–π)

## –°–∫—Ä–∏–ø—Ç–∏

- `setup.sh` - –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ (–≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π)
- `start.sh` - –ó–∞–ø—É—Å–∫ –æ–±–æ—Ö —Å–µ—Ä–≤–µ—Ä—ñ–≤ (—Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–æ)
- `stop.sh` - –ó—É–ø–∏–Ω–∫–∞ –≤—Å—ñ—Ö —Å–µ—Ä–≤–µ—Ä—ñ–≤
- `start_llama_server.sh` - –ó–∞–ø—É—Å–∫ —Ç—ñ–ª—å–∫–∏ llama server
- `start_proxy_server.sh` - –ó–∞–ø—É—Å–∫ —Ç—ñ–ª—å–∫–∏ proxy server

## –®–≤–∏–¥–∫–∏–π —Å—Ç–∞—Ä—Ç

```bash
# 1. –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è (–æ–¥–∏–Ω —Ä–∞–∑)
chmod +x setup.sh && ./setup.sh

# 2. –ó–∞–ø—É—Å–∫
./start.sh

# 3. –ó—É–ø–∏–Ω–∫–∞
./stop.sh
# –∞–±–æ Ctrl+C
```
